\documentclass[11pt, a4paper]{article}

% If you can't see cyrillic letters in R-studio choose
% File-Reopen with encoding
% utf8 is the preferred encoding


\input{title_bor_utf8_knitr}
\input{emetrix_preamble}




\usepackage[bibencoding = auto, backend = biber,
sorting = none]{biblatex}

\addbibresource{metrics_pro.bib}

\def \RR{\mathbb{R}}
\def \cN{\mathcal{N}}
\def \htheta{\hat{\theta}}

\title{Задачки для семинаров и домашки}
\author{Винни-Пух}
\date{\today}


% делаем короче интервал в списках
\setlength{\itemsep}{0pt}
\setlength{\parskip}{0pt}
\setlength{\parsep}{0pt}


\DeclareMathOperator{\Med}{Med}


\usepackage{answers} % дележка условий и ответов

%\newtheorem{problem}{Задача}
%\numberwithin{problem}{section}

\Newassociation{sol}{solution}{solution_file}
% sol — имя окружения внутри задач
% solution — имя окружения внутри solution_file
% solution_file — имя файла в который будет идти запись решений
% можно изменить далее по ходу
\Opensolutionfile{solution_file}[all_solutions]
% в квадратных скобках фактическое имя файла


% магия для автоматических гиперссылок задача-решение
\newlist{myenum}{enumerate}{3}
% \newcounter{problem}[chapter] % нумерация задач внутри глав
\newcounter{problem}

\newenvironment{problem}%
{%
\refstepcounter{problem}%
%  hyperlink to solution
     \hypertarget{problem:{\thesection.\theproblem}}{} % нумерация внутри глав
     % \hypertarget{problem:{\theproblem}}{}
     \Writetofile{solution_file}{\protect\hypertarget{soln:\thesection.\theproblem}{}}
     %\Writetofile{solution_file}{\protect\hypertarget{soln:\theproblem}{}}
     \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\thesection.\theproblem}{\thesection.\theproblem},ref=\thesection.\theproblem]
     % \begin{myenum}[label=\bfseries\protect\hyperlink{soln:\theproblem}{\theproblem},ref=\theproblem]
     \item%
    }%
    {%
    \end{myenum}}
% для гиперссылок обратно надо переопределять окружение
% это происходит непосредственно перед подключением файла с решениями





\begin{document}

% \maketitle % ставим сюда название, автора и время создания

\section{Линейная регрессия}

\begin{problem}
Рассмотрим задачу линейной регресии
\[
Q(w) = (y - Xw)^T(y - Xw) \to \min_{w}.
\]

\begin{enumerate}
\item Найдите $dQ(w)$ и $d^2Q(w)$.
\item Выведите формулу для оптимального $w$.
\item Выведите формулу для матрицы-шляпницы (hat-matrix), связывающей вектор фактических $y$ и вектор прогнозов $\hat y = H\cdot y$.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим задачу регрессии с одним признаком и без константы, $\hy_i = w \cdot x_i$. Решите в явном виде задачи МНК со штрафом:

\begin{enumerate}
\item $Q(w) = (y - \hy)^T (y - \hy) + \lambda w^2$;
\item $Q(w) = (y - \hy)^T (y - \hy) + \lambda |w|$;
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Храбрая и торопливая исследовательница Мишель хочет решить задачу линейной регрессии по $n$ наблюдениям с вектором $y$ и матрицей признаков $X$. Сначала исследовательница Мишель так торопилась, что совсем забыла последнее наблюдение и оценила задачу с более коротким вектором $y^{-}$ и матрицей $X^{-}$, где не хватает последней строки. Затем Мишель взяла правильную матрицу $X$, но неправильный вектор $y^*$, в котором она вместо фактического последнего наблюдения вектора $y$ вписала его прогноз, полученный с помощью регрессии с $y^{-1}$ и $X^{-}$.

\begin{enumerate}
\item Как связаны $\hat y_n^{-}$ и $\hat y_n^{*}$ (прогнозы для последнего наблюдения полученные по модели без последнего наблюдения и модели с неверным последним наблюдением)?
\item Как выглядит вектор, равный разнице $y - y^*$?
\item Какие величины находятся в векторе $H\cdot (y - y^*)$? Чему равна последняя, $n$-ая, компонента этого вектора? Выразите её через $H_{nn}$ и ошибку прогноза последнего наблюдения по модели без последнего наблюдения, $y_n - \hat y_n^{-}$.
\item Как связаны между собой ошибка прогноза $n$-го наблюдения по полной модели, ошибка прогноза $n$-го наблюдения по модели без последнего наблюдения и $H_{nn}$?
\item Как быстро провести кросс-валидацию с выкидыванием одного наблюдения для задачи линейной регрессии?
\end{enumerate}

\begin{sol}
\[
y_n - \hat y_n = (1 - H_{nn}) (y_n - \hat y_n^-)
\]
\end{sol}
\end{problem}


\section{Линейные классификаторы}

\begin{problem}
Рассмотрим плоскость в $\RR^3$, задаваемую уравнением $5x_1 + 6x_2 -7x_3 + 10 = 0$ и две точки, $A = (2, 1, 4)$ и $B = (4, 0, 4)$.
\begin{enumerate}
  \item Найдите любой вектор, перпендикулярный плоскости.
  \item Правда ли, что отрезок $AB$ пересекает плоскость?
  \item Найдите длину отрезка $AB$;
  \item Не находя расстояние от точек до плоскости, определите, во сколько раз точка $A$ дальше от плоскости, чем точка $B$;
  \item Найдите расстояние от точки $A$ до плоскости.
\end{enumerate}

\begin{sol}
$(5, 6, -7)$
\end{sol}
\end{problem}


\begin{problem}
Рассмотрим простейший персептрон с константой, единственным входом $x_1$ и пороговой функцией активации. Подберите веса так, чтобы персептрон реализовывал логическое отрицание (в ответ на 0 выдавал 1, и наоборот).
\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим простейший персептрон с константой, двумя входами $x_1$, $x_2$ и пороговой функцией активации.

\todo[inline]{Здесь ассистенты нарисуют в tikz картинку, достойную стоять вместо Джоконды в Лувре}

\begin{enumerate}
\item Подберите веса так, чтобы персептрон реализовывал логическое ИЛИ (OR).
\item Подберите веса так, чтобы персептрон реализовывал логическое И (AND).
\item Докажите, что веса невозможно подобрать так, чтобы персептрон реализовывал исключающее логическое ИЛИ (XOR).
\item Добавьте персептрону вход $x_3 = x_1 \cdot x_2$ и подберите веса так, чтобы персептрон реализовывал XOR.
\item Реализуйте XOR с помощью трёх персептронов с двумя входами и константой. Укажите веса и схему их взаимосвязей.
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
В коробке завалялось три персептрона, у каждого два входа с константой и пороговая функция активации. Реализуйте с их помощью функцию
\[
y = \begin{cases}
1, \text{ если } x_2 \geq |x_1 - 3| + 2; \\
0, \text{ иначе}
\end{cases}.
\]
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Рассмотрим следующий набор данных:

\begin{tabular}{ccc}
\toprule
$x_i$ & $z_i$ & $y_i$ \\
\midrule
-1 & -1 & 0 \\
1 & -1 & 0 \\
-1 & 1 & 0 \\
1 & 1 & 0 \\
0 & 2 & 1 \\
2 & 0 & 1 \\
0 & -2 & 1 \\
-2 & 0 & 1 \\
\bottomrule
\end{tabular}

\begin{enumerate}
\item Существует ли перспетрон с константой, двумя входами и пороговой функцией активации, способный идеально классифицировать $y_i$ на данной выборке? А хватит ли двух таких персептронов? А может хватит трёх?
\item Введите такое преобразование исходных признаков $h_i = h(x_i, z_i)$, при котором с идеальной классификацией $y_i$ справился бы даже персептрон с одним входом, константой и пороговой функцией активации.
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}





\begin{problem}
Бандерлог из Лога\footnote{деревня в Кадуйском районе Вологодской области} ведёт блог, любит считать логарифмы и оценивать логистические регрессии. С помощью нового алгоритма Бандерлог решил задачу классификации по трём наблюдениям и получил $b_i = \hat\P(y_i = 1|x_i)$.

\begin{tabular}{cc}
  \toprule
  $y_i$ & $b_i$ \\
  \midrule
  1 & 0.7 \\
  -1 & 0.2 \\
  -1 & 0.3 \\
  \bottomrule
\end{tabular}

\begin{enumerate}
\item Постройте ROC-кривую.
\item Найдите площадь под ROC-кривой и индекс Джини.
\item Постройте PR-кривую (кривая точность-полнота).
\item Найдите площадь под PR-кривой.
\item Как по-английски будет «бревно»?
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Классификатор Бандерлога имеет вид
\[
a_i = \begin{cases}
1, \text{ если } b_i > t; \\
-1, \text{ иначе.}
\end{cases}
\]

Докажите, что площадь под ROC-кривой равна вероятности того, случайно выбранный положительный объект окажется позже случайно выбранного отрицательного объекта, если объекты ранжированы по возрастанию величины $b_i$.
\begin{sol}
\end{sol}
\end{problem}






\begin{problem}
Все средние издалека выглядят одинаково, $\text{среднее}=f^{-1}(0.5f(x_1) + 0.5f(x_2))$.  Например, у среднего арифметического $f(t)=t$, у среднего гармонического $f(t)=1/t$.

\begin{enumerate}
  \item Какая $f$ используется для среднего геометрического?
\end{enumerate}

Для измерения качества бинарной классификации Ара использует среднее арифметическое точности и полноты, Гена — среднее геометрическое, а Гарик — среднее гармоническое.

\begin{enumerate}[resume]
  \item У кого будут выходить самые «качественные» и самые «некачественные» прогнозы?
\end{enumerate}


\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Бандерлог начинает все определения со слов «это доля правильных ответов»:
\begin{enumerate}
  \item accuracy — это доля правильных ответов\ldots
  \item точность (precision) — это доля правильных ответов\ldots
  \item полнота (recall) — это доля правильных ответов\ldots
  \item TPR — это доля правильных ответов\ldots
\end{enumerate}

Закончите определения Бандерлога так, чтобы они были, хм, правильными.
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Алгоритм бинарной классификации, придуманный Бандерлогом, выдаёт оценки вероятности $b_i = \hat\P(y_i=1 | x_i)$. Всего у Бандерлога 10000 наблюдений. Если ранжировать их по возрастанию $b_i$, то окажется что наблюдения с $y_i = 1$ занимают ровно места с  5501 по 5600.

Найдите площадь по ROC-кривой и площадь под PR-кривой.
\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
Бандерлог собрал выборку из 900 муравьёв и 100 китов. Переменная $y_i$ равна $1$ для китов. Бандерлог хочет, чтобы его алгоритм классификации выдавал для каждого наблюдения число $b_i=f(x_i) \in [0;1]$, оценку вероятности того, что наблюдение является китом. В качестве признака Бандерлог использует количество глаз, не задумавшись о том, что оно равно двум и для муравьёв, и для китов.

Решите задачу минимизации эмпирической функции риска и найдите все $b_i$ для функций потерь:
\begin{enumerate}
  \item $L(y_i, b_i) = (y_i - b_i)^2$, если для муравьёв $y_i = 0$;
  \item $L(y_i, b_i) = |y_i - b_i|$, если для муравьёв $y_i = 0$;
  \item $L(y_i, b_i) = \begin{cases}
  -\log b_i, \text{ если } y_i = 1 \\
  -\log (1-b_i), \text{ иначе.}
  \end{cases}$;
  %\item $L(y_i, b_i) = \log (1 + \exp(-y \cdot b))$, если для муравьёв $y = -1$;
  \item $L(y_i, b_i) = \begin{cases}
  1/b_i, \text{ если } y_i = 1 \\
  1/(1-b_i), \text{ иначе.}
  \end{cases}$;
\end{enumerate}
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
Бандерлог утверждает, что открыл новую верхнюю границу для пороговой функции потерь, $\tilde{L}(M_i) = 1 + \frac{1}{\pi} \cdot \arctan(-x_i)$, где $M_i = y_i \cdot \langle w, x_i \rangle$. Прав ли бандерлог?
\begin{sol}
  Нет. Не выполнено $\tilde{L} \geq L$ для всех $M \in \RR$.
\end{sol}
\end{problem}


\begin{problem}
Бандерлог из Лога оценил логистическую регрессию по четырём наблюдениям и одному признаку с константой, получил $b_i = \hat\P(y_i = 1|x_i)$, но потерял последнее наблюдение:

\begin{tabular}{cc}
  \toprule
  $y_i$ & $b_i$ \\
  \midrule
  1 & 0.7 \\
  -1 & 0.2 \\
  -1 & 0.3 \\
  ? &  ? \\
  \bottomrule
\end{tabular}

\begin{enumerate}
\item Выпишите функцию потерь для задачи логистической регрессии.
\item Выпишите условие первого порядка по коэффициенту перед константой.
\item Помогите бандерлогу восстановить пропущенные значения!
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
У Бандерлога три наблюдения, первое наблюдение — кит, остальные — муравьи. Киты кодируются $y_i = 1$, муравьи — $y_i = -1$. На этот раз Бандерлог, чтобы быть уверенным, что $x_i$ различаются, сам лично определил $x_i = i$. После этого Бандерлог оценивает логистическую регрессию с константой.

\begin{enumerate}
  \item Выпишите эмпирическую функцию риска, которую минимизирует Бандерлог;
  \item При каких оценках коэффициентов логистической регрессии эта функция достигает своего минимума?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}

\begin{problem}
Рассмотрим целевую функцию логистической регрессии с константой
\[
Q(w) = \frac{1}{\ell} \sum L(y_i, b_i),
\]
где $b_i = 1 / (1 + \exp( -\langle w, x_i\rangle)$ и $L(y_i, b_i) = \begin{cases}
-\log b_i, \text{ если } y_i = 1 \\
-\log (1-b_i), \text{ иначе.}
\end{cases}$.

\begin{enumerate}
\item Найдите $dQ(w)$ и $d^2Q(w)$;
\item Найдите $dQ(0)$ и $d^2Q(0)$;
\item Выпишите квадратичную аппроксимацию для $Q(w)$ в окрестности $w=0$;
\item С какой задачей совпадает задача минимизации квадратичной аппроксимации?
\end{enumerate}

\begin{sol}
\end{sol}
\end{problem}



\begin{problem}
ююю
\begin{sol}
\end{sol}
\end{problem}


\begin{problem}
ююю
\begin{sol}
\end{sol}
\end{problem}
% \section{Хочу ещё задач!}




\Closesolutionfile{solution_file}


% для гиперссылок на условия
% http://tex.stackexchange.com/questions/45415
\renewenvironment{solution}[1]{%
         % add some glue
         \vskip .5cm plus 2cm minus 0.1cm%
         {\bfseries \hyperlink{problem:#1}{#1.}}%
}%
{%
}%

%\section{Решения}
%\input{all_solutions}

%\section{Источники мудрости}
%\printbibliography[heading=none]


\end{document}
